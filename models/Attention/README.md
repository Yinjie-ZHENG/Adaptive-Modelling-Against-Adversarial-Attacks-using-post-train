# Attention



**Squeeze-and-Excitation(SE) Attention**

Paper: Squeeze-and-Excitation Networks,2017

Address：[https://arxiv.org/pdf/1709.01507.pdf](https://arxiv.org/pdf/1709.01507.pdf)

**External Attention**

Paper: Beyond Self-attention: External Attention using Two Linear Layers for Visual Tasks.---arXiv 2021.05.05

Address：[https://arxiv.org/abs/2105.02358](https://arxiv.org/abs/2105.02358)

**Self Attention**

Paper: Attention Is All You Need---NeurIPS2017

Address：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

**Selective Kernel(SK) Attention**

Paper: Selective Kernel Networks---CVPR2019

Address：[https://arxiv.org/pdf/1903.06586.pdf](https://arxiv.org/pdf/1903.06586.pdf)

**CBAM Attention**

Paper: CBAM: Convolutional Block Attention Module---ECCV2018

Address：[https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf](https://openaccess.thecvf.com/content_ECCV_2018/papers/Sanghyun_Woo_Convolutional_Block_Attention_ECCV_2018_paper.pdf)

**BAM Attention**

Paper: BAM: Bottleneck Attention Module---BMCV2018

Address：[https://arxiv.org/pdf/1807.06514.pdf](https://arxiv.org/pdf/1807.06514.pdf)

**ECA Attention**

Paper: ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks---CVPR2020

Address：[https://arxiv.org/pdf/1910.03151.pdf](https://arxiv.org/pdf/1910.03151.pdf)

**DANet Attention**

Paper: Dual Attention Network for Scene Segmentation---CVPR2019

Address：[https://arxiv.org/pdf/1809.02983.pdf](https://arxiv.org/pdf/1809.02983.pdf)

**Pyramid Split Attention(PSA)**

Paper: EPSANet: An Efficient Pyramid Split Attention Block on Convolutional Neural Network---arXiv 2021.05.30

Address：[https://arxiv.org/pdf/2105.14447.pdf](https://arxiv.org/pdf/2105.14447.pdf)

**Efficient Multi-Head Self-Attention(EMSA)**

Paper: ResT: An Efficient Transformer for Visual Recognition---arXiv 2021.05.28

Address：[https://arxiv.org/abs/2105.13677](https://arxiv.org/abs/2105.13677)

**cscSENet**

Paper: Concurrent Spatial and Channel Squeeze & Excitation in Fully Convolutional Networks, 2018

Address：[https://liumin.blog.csdn.net/article/details/104371065](https://liumin.blog.csdn.net/article/details/104371065)

**Non-Local Attention**

Paper: Non-Local neural networks,2018

Address：[https://arxiv.org/pdf/1711.07971.pdf](https://arxiv.org/pdf/1711.07971.pdf)

**GCNet**

Paper: GCNet: Non-local Networks Meet Squeeze-Excitation Networks and Beyond, 2019

Address：[https://arxiv.org/pdf/1904.11492.pdf](https://arxiv.org/pdf/1904.11492.pdf)

**Other Attention**
- **A2Attention**
["A2-Nets: Double Attention Networks---NIPS2018"](https://arxiv.org/pdf/1810.11579.pdf)
- **CoAtNet**
[CoAtNet: Marrying Convolution and Attention for All Data Sizes---arXiv 2021.06.09](https://arxiv.org/abs/2106.04803) 
- **CoordAttention**
[Coordinate Attention for Efficient Mobile Network Design ---CVPR 2021](https://arxiv.org/abs/2103.02907)
- **HaloAttention**
[Scaling Local Self-Attention for Parameter Efficient Visual Backbones"](https://arxiv.org/pdf/2103.12731.pdf) 
- **MobileViTAttention**
[MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer---ArXiv 2021.10.05](https://arxiv.org/abs/2103.02907)
- **MUSEAttention**  
["MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning---arXiv 2019.11.17"](https://arxiv.org/abs/1911.09483)
- **OutlookAttention**
[VOLO: Vision Outlooker for Visual Recognition---arXiv 2021.06.24"](https://arxiv.org/abs/2106.13112) 
- **ParNetAttention**
[Non-deep Networks---ArXiv 2021.10.20](https://arxiv.org/abs/2110.07641)
- **ParallelPolarizedSelfAttention**
[Polarized Self-Attention: Towards High-quality Pixel-wise Regression---arXiv 2021.07.02](https://arxiv.org/abs/2107.00782)
- **residual_attention**
[Residual Attention: A Simple but Effective Method for Multi-Label Recognition---ICCV2021](https://arxiv.org/abs/2108.02456) 
- **S2Attention**
[S²-MLPv2: Improved Spatial-Shift MLP Architecture for Vision---arXiv 2021.08.02](https://arxiv.org/abs/2108.01072)
- **SpatialGroupEnhance Attention**
["Spatial Group-wise Enhance: Improving Semantic Feature Learning in Convolutional Networks---arXiv 2019.05.23"](https://arxiv.org/pdf/1905.09646.pdf)
- **ShuffleAttention**
["SA-NET: SHUFFLE ATTENTION FOR DEEP CONVOLUTIONAL NEURAL NETWORKS---ICASSP 2021"](https://arxiv.org/pdf/2102.00240.pdf)
- **GFNet Attention**
[Global Filter Networks for Image Classification---arXiv 2021.07.01](https://arxiv.org/abs/2107.00645) 
- **TripletAttention**
[Rotate to Attend: Convolutional Triplet Attention Module---WACV 2021](https://arxiv.org/abs/2010.03045) 
- **UFOAttention**
[UFO-ViT: High Performance Linear Vision Transformer without Softmax---ArXiv 2021.09.29](https://arxiv.org/abs/2110.07641)
- **VIPAttention**
[Vision Permutator: A Permutable MLP-Like Architecture for Visual Recognition---arXiv 2021.06.23](https://arxiv.org/abs/2106.12368) 



## Write at the end
At present, the Attention work organized by this project is indeed not comprehensive enough. As the amount of reading increases, we will continue to improve this project. Welcome everyone star to support. If there are incorrect statements or incorrect code implementations in the article, you are welcome to point out~


